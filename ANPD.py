# -*- coding: utf-8 -*-
"""Number plate detection using RNN, CNN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DAyc-cdQXvsYkIMPQj3AAPM1MkBlpXeY
"""

!pip install imutils
!pip install easyocr
!pip install tensorflow
!pip install opencv-python

from google.colab import drive
drive.mount('/content/drive')

import cv2
from matplotlib import pyplot as plt
import numpy as np
import imutils
import tensorflow as tf
import easyocr
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LSTM, Bidirectional, TimeDistributed
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Input

img = cv2.imread('/content/drive/MyDrive/image3.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
plt.imshow(cv2.cvtColor(gray, cv2.COLOR_BGR2RGB))

bfilter = cv2.bilateralFilter(gray, 11, 17, 17)  # Noise reduction
edged = cv2.Canny(bfilter, 30, 200)  # Edge detection
plt.imshow(cv2.cvtColor(edged, cv2.COLOR_BGR2RGB))

keypoints = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
contours = imutils.grab_contours(keypoints)
contours = sorted(contours, key=cv2.contourArea, reverse=True)[:10]

location = None
for contour in contours:
    approx = cv2.approxPolyDP(contour, 10, True)
    if len(approx) == 4:
        location = approx
        break

location

mask = np.zeros(gray.shape, np.uint8)
new_image = cv2.drawContours(mask, [location], 0, 255, -1)
new_image = cv2.bitwise_and(img, img, mask=mask)

(x, y) = np.where(mask == 255)
(x1, y1) = (np.min(x), np.min(y))
(x2, y2) = (np.max(x), np.max(y))
cropped_image = gray[x1:x2+1, y1:y2+1]

cropped_image_resized = cv2.resize(cropped_image, (128, 64)) / 255.0
cropped_image_resized = np.expand_dims(cropped_image_resized, axis=-1)

# Step 2: Define CNN Model for Feature Extraction
def create_cnn(input_shape):
    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        MaxPooling2D((2, 2)),
        Conv2D(64, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        Conv2D(128, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        Flatten(),
        Dense(128, activation='relu')
    ])
    return model

# Instantiate CNN model
cnn_model = create_cnn((64, 128, 1))

# Extract features from the cropped image
features = cnn_model.predict(np.expand_dims(cropped_image_resized, axis=0))

# Step 3: Define RNN Model for Character Recognition
def create_rnn(input_dim, time_steps, num_classes):
    model = Sequential([
        Input(shape=(time_steps, input_dim)),
        Bidirectional(LSTM(128, return_sequences=True)),
        Bidirectional(LSTM(128, return_sequences=True)),
        TimeDistributed(Dense(num_classes, activation='softmax'))
    ])
    return model

# Define the RNN model
time_steps = 10  # Maximum number of characters in the license plate
num_classes = 36  # 10 digits + 26 alphabets
rnn_model = create_rnn(128, time_steps, num_classes)

# Predict sequence of characters (simulate input for demonstration)
dummy_sequence = np.tile(features, (time_steps, 1))  # Repeat features for RNN input
predictions = rnn_model.predict(np.expand_dims(dummy_sequence, axis=0))

# Predict sequence of characters (simulate input for demonstration)
dummy_sequence = np.tile(features, (time_steps, 1))  # Repeat features for RNN input
predictions = rnn_model.predict(np.expand_dims(dummy_sequence, axis=0))

reader = easyocr.Reader(['en'])
result = reader.readtext(cropped_image)
result

# Step 4: Annotate the Image with the Predicted Text
font = cv2.FONT_HERSHEY_SIMPLEX
text =  result[0][-2]
res = cv2.putText(img, text=text, org=(approx[0][0][0], approx[1][0][1] + 60), fontFace=font, fontScale=1, color=(0, 255, 0), thickness=2, lineType=cv2.LINE_AA)
res = cv2.rectangle(img, tuple(approx[0][0]), tuple(approx[2][0]), (0, 255, 0), 3)

plt.imshow(cv2.cvtColor(res, cv2.COLOR_BGR2RGB))
plt.show()

